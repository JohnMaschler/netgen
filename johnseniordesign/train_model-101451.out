
The following have been reloaded with a version change:
  1) GCCcore/12.3.0 => GCCcore/14.2.0
  2) binutils/2.40-GCCcore-12.3.0 => binutils/2.42-GCCcore-14.2.0
  3) zlib/1.2.13-GCCcore-12.3.0 => zlib/1.3.1-GCCcore-14.2.0

Activating virtual environment...
Using Python from: /WAVE/projects2/sd-2024-25-cloud-lab-llm/NetGen/john/env/bin/python
Starting training model5...
Number of trainable parameters: 86446572
Epoch [1/300], Loss: 10.031887
Epoch [2/300], Loss: 8.732256
Epoch [3/300], Loss: 7.735600
Epoch [4/300], Loss: 7.140633
Epoch [5/300], Loss: 6.562522
Epoch [6/300], Loss: 6.110704
Epoch [7/300], Loss: 5.788904
Epoch [8/300], Loss: 5.564532
Epoch [9/300], Loss: 5.373101
Epoch [10/300], Loss: 5.215873
Epoch [11/300], Loss: 5.100363
Epoch [12/300], Loss: 5.006348
Epoch [13/300], Loss: 4.928007
Epoch [14/300], Loss: 4.855202
Epoch [15/300], Loss: 4.790430
Epoch [16/300], Loss: 4.728993
Epoch [17/300], Loss: 4.685079
Epoch [18/300], Loss: 4.632876
Epoch [19/300], Loss: 4.587474
Epoch [20/300], Loss: 4.543283
Epoch [21/300], Loss: 4.508887
Epoch [22/300], Loss: 4.470376
Epoch [23/300], Loss: 4.435451
Epoch [24/300], Loss: 4.402562
Epoch [25/300], Loss: 4.374827
Epoch [26/300], Loss: 4.342289
Epoch [27/300], Loss: 4.311275
Epoch [28/300], Loss: 4.287798
Epoch [29/300], Loss: 4.265037
Epoch [30/300], Loss: 4.236892
Epoch [31/300], Loss: 4.211328
Epoch [32/300], Loss: 4.193085
Epoch [33/300], Loss: 4.171546
Epoch [34/300], Loss: 4.145144
Epoch [35/300], Loss: 4.131635
Epoch [36/300], Loss: 4.113729
Epoch [37/300], Loss: 4.086341
Epoch [38/300], Loss: 4.073834
Epoch [39/300], Loss: 4.057899
Epoch [40/300], Loss: 4.037456
Epoch [41/300], Loss: 4.020403
Epoch [42/300], Loss: 4.007958
Epoch [43/300], Loss: 3.991649
Epoch [44/300], Loss: 3.980432
Epoch [45/300], Loss: 3.961514
Epoch [46/300], Loss: 3.944191
Epoch [47/300], Loss: 3.936730
Epoch [48/300], Loss: 3.925782
Epoch [49/300], Loss: 3.907395
Epoch [50/300], Loss: 3.892417
Epoch [51/300], Loss: 3.878278
Epoch [52/300], Loss: 3.868486
Epoch [53/300], Loss: 3.855744
Epoch [54/300], Loss: 3.843716
Epoch [55/300], Loss: 3.832411
Epoch [56/300], Loss: 3.819281
Epoch [57/300], Loss: 3.809193
Epoch [58/300], Loss: 3.797133
Epoch [59/300], Loss: 3.782116
Epoch [60/300], Loss: 3.777285
Epoch [61/300], Loss: 3.765680
Epoch [62/300], Loss: 3.760438
Epoch [63/300], Loss: 3.747086
Epoch [64/300], Loss: 3.742047
Epoch [65/300], Loss: 3.728876
Epoch [66/300], Loss: 3.720885
Epoch [67/300], Loss: 3.707388
Epoch [68/300], Loss: 3.696756
Epoch [69/300], Loss: 3.689663
Epoch [70/300], Loss: 3.682575
Epoch [71/300], Loss: 3.674030
Epoch [72/300], Loss: 3.672729
Epoch [73/300], Loss: 3.653977
Epoch [74/300], Loss: 3.649605
Epoch [75/300], Loss: 3.646364
Epoch [76/300], Loss: 3.637683
Epoch [77/300], Loss: 3.628330
Epoch [78/300], Loss: 3.621140
Epoch [79/300], Loss: 3.610680
Epoch [80/300], Loss: 3.608390
Epoch [81/300], Loss: 3.595005
Epoch [82/300], Loss: 3.593110
Epoch [83/300], Loss: 3.585711
Epoch [84/300], Loss: 3.579024
Epoch [85/300], Loss: 3.565574
Epoch [86/300], Loss: 3.565897
Epoch [87/300], Loss: 3.556335
Epoch [88/300], Loss: 3.555614
Epoch [89/300], Loss: 3.539914
Epoch [90/300], Loss: 3.539018
Epoch [91/300], Loss: 3.538548
Epoch [92/300], Loss: 3.521801
Epoch [93/300], Loss: 3.520070
Epoch [94/300], Loss: 3.512670
Epoch [95/300], Loss: 3.507350
Epoch [96/300], Loss: 3.502457
Epoch [97/300], Loss: 3.495287
Epoch [98/300], Loss: 3.485628
Epoch [99/300], Loss: 3.477778
Epoch [100/300], Loss: 3.478711
Epoch [101/300], Loss: 3.471443
Epoch [102/300], Loss: 3.467520
Epoch [103/300], Loss: 3.463331
Epoch [104/300], Loss: 3.460754
Epoch [105/300], Loss: 3.448184
Epoch [106/300], Loss: 3.447729
Epoch [107/300], Loss: 3.439331
Epoch [108/300], Loss: 3.435069
Epoch [109/300], Loss: 3.430904
Epoch [110/300], Loss: 3.424879
Epoch [111/300], Loss: 3.427215
Epoch [112/300], Loss: 3.416393
Epoch [113/300], Loss: 3.410972
Epoch [114/300], Loss: 3.410283
Epoch [115/300], Loss: 3.404380
Epoch [116/300], Loss: 3.397098
Epoch [117/300], Loss: 3.391286
Epoch [118/300], Loss: 3.391798
Epoch [119/300], Loss: 3.383287
Epoch [120/300], Loss: 3.371449
Epoch [121/300], Loss: 3.381464
Epoch [122/300], Loss: 3.364357
Epoch [123/300], Loss: 3.369835
Epoch [124/300], Loss: 3.363258
Epoch [125/300], Loss: 3.358413
Epoch [126/300], Loss: 3.347588
Epoch [127/300], Loss: 3.341757
Epoch [128/300], Loss: 3.338044
Epoch [129/300], Loss: 3.332566
Epoch [130/300], Loss: 3.327033
Epoch [131/300], Loss: 3.326419
Epoch [132/300], Loss: 3.326472
Epoch [133/300], Loss: 3.320788
Epoch [134/300], Loss: 3.314060
Epoch [135/300], Loss: 3.306318
Epoch [136/300], Loss: 3.308334
Epoch [137/300], Loss: 3.302223
Epoch [138/300], Loss: 3.299592
Epoch [139/300], Loss: 3.298631
Epoch [140/300], Loss: 3.286117
Epoch [141/300], Loss: 3.291465
Epoch [142/300], Loss: 3.280855
Epoch [143/300], Loss: 3.276248
Epoch [144/300], Loss: 3.276671
Epoch [145/300], Loss: 3.271559
Epoch [146/300], Loss: 3.263724
Epoch [147/300], Loss: 3.268271
Epoch [148/300], Loss: 3.257955
Epoch [149/300], Loss: 3.264822
Epoch [150/300], Loss: 3.252459
Epoch [151/300], Loss: 3.256271
Epoch [152/300], Loss: 3.246883
Epoch [153/300], Loss: 3.252077
Epoch [154/300], Loss: 3.245268
Epoch [155/300], Loss: 3.238111
Epoch [156/300], Loss: 3.234110
Epoch [157/300], Loss: 3.233115
Epoch [158/300], Loss: 3.231205
Epoch [159/300], Loss: 3.224585
Epoch [160/300], Loss: 3.226097
Epoch [161/300], Loss: 3.214453
Epoch [162/300], Loss: 3.209473
Epoch [163/300], Loss: 3.205696
Epoch [164/300], Loss: 3.208382
Epoch [165/300], Loss: 3.203172
Epoch [166/300], Loss: 3.203186
Epoch [167/300], Loss: 3.200026
Epoch [168/300], Loss: 3.189065
Epoch [169/300], Loss: 3.188749
Epoch [170/300], Loss: 3.187363
Epoch [171/300], Loss: 3.180092
Epoch [172/300], Loss: 3.173057
Epoch [173/300], Loss: 3.180117
Epoch [174/300], Loss: 3.173422
Epoch [175/300], Loss: 3.170423
Epoch [176/300], Loss: 3.175922
Epoch [177/300], Loss: 3.165508
Epoch [178/300], Loss: 3.164867
Epoch [179/300], Loss: 3.153669
Epoch [180/300], Loss: 3.154436
Epoch [181/300], Loss: 3.151298
Epoch [182/300], Loss: 3.152490
Epoch [183/300], Loss: 3.144314
Epoch [184/300], Loss: 3.145476
Epoch [185/300], Loss: 3.144379
Epoch [186/300], Loss: 3.140524
Epoch [187/300], Loss: 3.136125
Epoch [188/300], Loss: 3.132835
Epoch [189/300], Loss: 3.134651
Epoch [190/300], Loss: 3.129270
Epoch [191/300], Loss: 3.123355
Epoch [192/300], Loss: 3.124348
Epoch [193/300], Loss: 3.116045
Epoch [194/300], Loss: 3.122778
Epoch [195/300], Loss: 3.111019
Epoch [196/300], Loss: 3.110257
Epoch [197/300], Loss: 3.114073
Epoch [198/300], Loss: 3.103860
Epoch [199/300], Loss: 3.109962
Epoch [200/300], Loss: 3.098403
Epoch [201/300], Loss: 3.096994
Epoch [202/300], Loss: 3.089325
Epoch [203/300], Loss: 3.094138
Epoch [204/300], Loss: 3.094392
Epoch [205/300], Loss: 3.095230
Epoch [206/300], Loss: 3.085812
Epoch [207/300], Loss: 3.087609
Epoch [208/300], Loss: 3.082155
Epoch [209/300], Loss: 3.073466
Epoch [210/300], Loss: 3.077057
Epoch [211/300], Loss: 3.070857
Epoch [212/300], Loss: 3.069414
Epoch [213/300], Loss: 3.075623
Epoch [214/300], Loss: 3.062133
Epoch [215/300], Loss: 3.057802
Epoch [216/300], Loss: 3.063662
Epoch [217/300], Loss: 3.059933
Epoch [218/300], Loss: 3.060966
Epoch [219/300], Loss: 3.062022
Epoch [220/300], Loss: 3.046573
Epoch [221/300], Loss: 3.049088
Epoch [222/300], Loss: 3.044404
Epoch [223/300], Loss: 3.039649
Epoch [224/300], Loss: 3.047313
Epoch [225/300], Loss: 3.040577
Epoch [226/300], Loss: 3.040020
Epoch [227/300], Loss: 3.034074
Epoch [228/300], Loss: 3.032675
Epoch [229/300], Loss: 3.029302
Epoch [230/300], Loss: 3.029755
Epoch [231/300], Loss: 3.024653
Epoch [232/300], Loss: 3.022930
Epoch [233/300], Loss: 3.022665
Epoch [234/300], Loss: 3.021626
Epoch [235/300], Loss: 3.020921
Epoch [236/300], Loss: 3.012931
Epoch [237/300], Loss: 3.013357
Epoch [238/300], Loss: 3.008425
Epoch [239/300], Loss: 3.011590
Epoch [240/300], Loss: 3.009281
Epoch [241/300], Loss: 3.004137
Epoch [242/300], Loss: 2.995573
Epoch [243/300], Loss: 3.001034
Epoch [244/300], Loss: 2.999386
Epoch [245/300], Loss: 2.996609
Epoch [246/300], Loss: 2.990372
Epoch [247/300], Loss: 2.990431
Epoch [248/300], Loss: 2.986525
Epoch [249/300], Loss: 2.989666
Epoch [250/300], Loss: 2.988275
Epoch [251/300], Loss: 2.982228
Epoch [252/300], Loss: 2.984578
Epoch [253/300], Loss: 2.978002
Epoch [254/300], Loss: 2.980851
Epoch [255/300], Loss: 2.970524
Epoch [256/300], Loss: 2.972861
Epoch [257/300], Loss: 2.964656
Epoch [258/300], Loss: 2.973766
Epoch [259/300], Loss: 2.965660
Epoch [260/300], Loss: 2.967202
Epoch [261/300], Loss: 2.964883
Epoch [262/300], Loss: 2.958762
Epoch [263/300], Loss: 2.957859
Epoch [264/300], Loss: 2.960944
Epoch [265/300], Loss: 2.958023
Epoch [266/300], Loss: 2.948443
Epoch [267/300], Loss: 2.953339
Epoch [268/300], Loss: 2.945100
Epoch [269/300], Loss: 2.942532
Epoch [270/300], Loss: 2.944190
Epoch [271/300], Loss: 2.945202
Epoch [272/300], Loss: 2.938589
Epoch [273/300], Loss: 2.940034
Epoch [274/300], Loss: 2.935248
Epoch [275/300], Loss: 2.935186
Epoch [276/300], Loss: 2.931567
Epoch [277/300], Loss: 2.927407
Epoch [278/300], Loss: 2.929789
Epoch [279/300], Loss: 2.923164
Epoch [280/300], Loss: 2.923312
Epoch [281/300], Loss: 2.926362
Epoch [282/300], Loss: 2.918755
Epoch [283/300], Loss: 2.922577
Epoch [284/300], Loss: 2.920639
Epoch [285/300], Loss: 2.915793
Epoch [286/300], Loss: 2.918788
Epoch [287/300], Loss: 2.909335
Epoch [288/300], Loss: 2.911617
Epoch [289/300], Loss: 2.909496
Epoch [290/300], Loss: 2.910245
Epoch [291/300], Loss: 2.902466
Epoch [292/300], Loss: 2.908090
Epoch [293/300], Loss: 2.901479
Epoch [294/300], Loss: 2.902976
Epoch [295/300], Loss: 2.907613
Epoch [296/300], Loss: 2.898605
Epoch [297/300], Loss: 2.894213
Epoch [298/300], Loss: 2.895424
Epoch [299/300], Loss: 2.888112
Epoch [300/300], Loss: 2.891399
Traceback (most recent call last):
  File "train_model5.py", line 356, in <module>
    main()
  File "train_model5.py", line 346, in main
    generated_tokens = generate_packets(model, user_prompt, text_tokenizer, packet_tokenizer, device, max_len=100)
  File "train_model5.py", line 231, in generate_packets
    top_k_filtered_logits = top_k_logits(logits, k)
  File "train_model5.py", line 212, in top_k_logits
    return torch.where(logits < min_values, float('-inf'), logits)
RuntimeError: expected scalar type double but found float
