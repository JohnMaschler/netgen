
The following have been reloaded with a version change:
  1) GCCcore/12.3.0 => GCCcore/14.2.0
  2) binutils/2.40-GCCcore-12.3.0 => binutils/2.42-GCCcore-14.2.0
  3) zlib/1.2.13-GCCcore-12.3.0 => zlib/1.3.1-GCCcore-14.2.0

Activating virtual environment...
Using Python from: /WAVE/projects2/sd-2024-25-cloud-lab-llm/NetGen/john/env/bin/python
Starting training model5...
Number of trainable parameters: 86446572
Epoch [1/300], Loss: 10.195357
Epoch [2/300], Loss: 8.915438
Epoch [3/300], Loss: 7.834269
Epoch [4/300], Loss: 7.173873
Epoch [5/300], Loss: 6.567657
Epoch [6/300], Loss: 6.111401
Epoch [7/300], Loss: 5.800303
Epoch [8/300], Loss: 5.567662
Epoch [9/300], Loss: 5.373454
Epoch [10/300], Loss: 5.216280
Epoch [11/300], Loss: 5.093690
Epoch [12/300], Loss: 5.008526
Epoch [13/300], Loss: 4.916433
Epoch [14/300], Loss: 4.843974
Epoch [15/300], Loss: 4.780253
Epoch [16/300], Loss: 4.718686
Epoch [17/300], Loss: 4.668117
Epoch [18/300], Loss: 4.620680
Epoch [19/300], Loss: 4.577368
Epoch [20/300], Loss: 4.530484
Epoch [21/300], Loss: 4.494327
Epoch [22/300], Loss: 4.461016
Epoch [23/300], Loss: 4.414100
Epoch [24/300], Loss: 4.385589
Epoch [25/300], Loss: 4.362358
Epoch [26/300], Loss: 4.331531
Epoch [27/300], Loss: 4.303222
Epoch [28/300], Loss: 4.279897
Epoch [29/300], Loss: 4.248270
Epoch [30/300], Loss: 4.225738
Epoch [31/300], Loss: 4.203927
Epoch [32/300], Loss: 4.174861
Epoch [33/300], Loss: 4.158342
Epoch [34/300], Loss: 4.141947
Epoch [35/300], Loss: 4.116860
Epoch [36/300], Loss: 4.100992
Epoch [37/300], Loss: 4.075383
Epoch [38/300], Loss: 4.065190
Epoch [39/300], Loss: 4.048288
Epoch [40/300], Loss: 4.027029
Epoch [41/300], Loss: 4.006555
Epoch [42/300], Loss: 4.004277
Epoch [43/300], Loss: 3.985200
Epoch [44/300], Loss: 3.966852
Epoch [45/300], Loss: 3.948418
Epoch [46/300], Loss: 3.943746
Epoch [47/300], Loss: 3.927783
Epoch [48/300], Loss: 3.912442
Epoch [49/300], Loss: 3.899129
Epoch [50/300], Loss: 3.884624
Epoch [51/300], Loss: 3.876297
Epoch [52/300], Loss: 3.864478
Epoch [53/300], Loss: 3.850115
Epoch [54/300], Loss: 3.842436
Epoch [55/300], Loss: 3.832154
Epoch [56/300], Loss: 3.821952
Epoch [57/300], Loss: 3.803804
Epoch [58/300], Loss: 3.801187
Epoch [59/300], Loss: 3.787983
Epoch [60/300], Loss: 3.774150
Epoch [61/300], Loss: 3.768659
Epoch [62/300], Loss: 3.757104
Epoch [63/300], Loss: 3.746932
Epoch [64/300], Loss: 3.740280
Epoch [65/300], Loss: 3.724002
Epoch [66/300], Loss: 3.725056
Epoch [67/300], Loss: 3.715379
Epoch [68/300], Loss: 3.699668
Epoch [69/300], Loss: 3.693197
Epoch [70/300], Loss: 3.685039
Epoch [71/300], Loss: 3.678654
Epoch [72/300], Loss: 3.666176
Epoch [73/300], Loss: 3.658663
Epoch [74/300], Loss: 3.655105
Epoch [75/300], Loss: 3.643891
Epoch [76/300], Loss: 3.637157
Epoch [77/300], Loss: 3.624412
Epoch [78/300], Loss: 3.616661
Epoch [79/300], Loss: 3.612541
Epoch [80/300], Loss: 3.606068
Epoch [81/300], Loss: 3.594744
Epoch [82/300], Loss: 3.595102
Epoch [83/300], Loss: 3.584106
Epoch [84/300], Loss: 3.583375
Epoch [85/300], Loss: 3.567957
Epoch [86/300], Loss: 3.564905
Epoch [87/300], Loss: 3.559471
Epoch [88/300], Loss: 3.543494
Epoch [89/300], Loss: 3.546165
Epoch [90/300], Loss: 3.537774
Epoch [91/300], Loss: 3.535597
Epoch [92/300], Loss: 3.529700
Epoch [93/300], Loss: 3.517855
Epoch [94/300], Loss: 3.506551
Epoch [95/300], Loss: 3.507279
Epoch [96/300], Loss: 3.507264
Epoch [97/300], Loss: 3.491923
Epoch [98/300], Loss: 3.492875
Epoch [99/300], Loss: 3.478161
Epoch [100/300], Loss: 3.474209
Epoch [101/300], Loss: 3.469149
Epoch [102/300], Loss: 3.468772
Epoch [103/300], Loss: 3.461521
Epoch [104/300], Loss: 3.457144
Epoch [105/300], Loss: 3.445676
Epoch [106/300], Loss: 3.445007
Epoch [107/300], Loss: 3.439562
Epoch [108/300], Loss: 3.432078
Epoch [109/300], Loss: 3.430803
Epoch [110/300], Loss: 3.427240
Epoch [111/300], Loss: 3.417875
Epoch [112/300], Loss: 3.410599
Epoch [113/300], Loss: 3.407560
Epoch [114/300], Loss: 3.407230
Epoch [115/300], Loss: 3.403737
Epoch [116/300], Loss: 3.393586
Epoch [117/300], Loss: 3.384379
Epoch [118/300], Loss: 3.380643
Epoch [119/300], Loss: 3.382422
Epoch [120/300], Loss: 3.373056
Epoch [121/300], Loss: 3.372389
Epoch [122/300], Loss: 3.358110
Epoch [123/300], Loss: 3.366776
Epoch [124/300], Loss: 3.362953
Epoch [125/300], Loss: 3.355253
Epoch [126/300], Loss: 3.346538
Epoch [127/300], Loss: 3.340410
Epoch [128/300], Loss: 3.333346
Epoch [129/300], Loss: 3.333382
Epoch [130/300], Loss: 3.324971
Epoch [131/300], Loss: 3.328513
Epoch [132/300], Loss: 3.319999
Epoch [133/300], Loss: 3.324550
Epoch [134/300], Loss: 3.310780
Epoch [135/300], Loss: 3.307891
Epoch [136/300], Loss: 3.309873
Epoch [137/300], Loss: 3.302300
Epoch [138/300], Loss: 3.297503
Epoch [139/300], Loss: 3.293397
Epoch [140/300], Loss: 3.292092
Epoch [141/300], Loss: 3.296219
Epoch [142/300], Loss: 3.282834
Epoch [143/300], Loss: 3.287022
Epoch [144/300], Loss: 3.280339
Epoch [145/300], Loss: 3.267910
Epoch [146/300], Loss: 3.268794
Epoch [147/300], Loss: 3.266409
Epoch [148/300], Loss: 3.252507
Epoch [149/300], Loss: 3.255127
Epoch [150/300], Loss: 3.250000
Epoch [151/300], Loss: 3.252024
Epoch [152/300], Loss: 3.251393
Epoch [153/300], Loss: 3.242056
Epoch [154/300], Loss: 3.247182
Epoch [155/300], Loss: 3.239591
Epoch [156/300], Loss: 3.228136
Epoch [157/300], Loss: 3.223507
Epoch [158/300], Loss: 3.226722
Epoch [159/300], Loss: 3.223702
Epoch [160/300], Loss: 3.213293
Epoch [161/300], Loss: 3.215192
Epoch [162/300], Loss: 3.209561
Epoch [163/300], Loss: 3.212600
Epoch [164/300], Loss: 3.206790
Epoch [165/300], Loss: 3.205617
Epoch [166/300], Loss: 3.199659
Epoch [167/300], Loss: 3.192359
Epoch [168/300], Loss: 3.191621
Epoch [169/300], Loss: 3.186154
Epoch [170/300], Loss: 3.182388
Epoch [171/300], Loss: 3.187085
Epoch [172/300], Loss: 3.179304
Epoch [173/300], Loss: 3.176659
Epoch [174/300], Loss: 3.169421
Epoch [175/300], Loss: 3.162139
Epoch [176/300], Loss: 3.171909
Epoch [177/300], Loss: 3.164703
Epoch [178/300], Loss: 3.168781
Epoch [179/300], Loss: 3.165712
Epoch [180/300], Loss: 3.159427
Epoch [181/300], Loss: 3.153868
Epoch [182/300], Loss: 3.150813
Epoch [183/300], Loss: 3.146733
Epoch [184/300], Loss: 3.148735
Epoch [185/300], Loss: 3.144069
Epoch [186/300], Loss: 3.135436
Epoch [187/300], Loss: 3.133926
Epoch [188/300], Loss: 3.133555
Epoch [189/300], Loss: 3.133940
Epoch [190/300], Loss: 3.125591
Epoch [191/300], Loss: 3.131460
Epoch [192/300], Loss: 3.119010
Epoch [193/300], Loss: 3.122682
Epoch [194/300], Loss: 3.112807
Epoch [195/300], Loss: 3.117482
Epoch [196/300], Loss: 3.109063
Epoch [197/300], Loss: 3.106386
Epoch [198/300], Loss: 3.110569
Epoch [199/300], Loss: 3.106995
Epoch [200/300], Loss: 3.101442
Epoch [201/300], Loss: 3.100044
Epoch [202/300], Loss: 3.100282
Epoch [203/300], Loss: 3.097180
Epoch [204/300], Loss: 3.085886
Epoch [205/300], Loss: 3.094302
Epoch [206/300], Loss: 3.083418
Epoch [207/300], Loss: 3.079218
Epoch [208/300], Loss: 3.088045
Epoch [209/300], Loss: 3.071273
Epoch [210/300], Loss: 3.078520
Epoch [211/300], Loss: 3.074308
Epoch [212/300], Loss: 3.069199
Epoch [213/300], Loss: 3.065258
Epoch [214/300], Loss: 3.071036
Epoch [215/300], Loss: 3.064877
Epoch [216/300], Loss: 3.060245
Epoch [217/300], Loss: 3.055757
Epoch [218/300], Loss: 3.059826
Epoch [219/300], Loss: 3.063087
Epoch [220/300], Loss: 3.058587
Epoch [221/300], Loss: 3.052454
Epoch [222/300], Loss: 3.052919
Epoch [223/300], Loss: 3.048352
Epoch [224/300], Loss: 3.038685
Epoch [225/300], Loss: 3.042684
Epoch [226/300], Loss: 3.039489
Epoch [227/300], Loss: 3.033242
Epoch [228/300], Loss: 3.033283
Epoch [229/300], Loss: 3.036705
Epoch [230/300], Loss: 3.027848
Epoch [231/300], Loss: 3.024645
Epoch [232/300], Loss: 3.022478
Epoch [233/300], Loss: 3.029464
Epoch [234/300], Loss: 3.019770
Epoch [235/300], Loss: 3.022812
Epoch [236/300], Loss: 3.014022
Epoch [237/300], Loss: 3.016700
Epoch [238/300], Loss: 3.018481
Epoch [239/300], Loss: 3.007709
Epoch [240/300], Loss: 3.010559
Epoch [241/300], Loss: 3.009064
Epoch [242/300], Loss: 3.004185
Epoch [243/300], Loss: 3.001987
Epoch [244/300], Loss: 2.998630
Epoch [245/300], Loss: 2.993867
Epoch [246/300], Loss: 2.990936
Epoch [247/300], Loss: 2.989992
Epoch [248/300], Loss: 2.990301
Epoch [249/300], Loss: 2.995276
Epoch [250/300], Loss: 2.985922
Epoch [251/300], Loss: 2.988530
Epoch [252/300], Loss: 2.983436
Epoch [253/300], Loss: 2.981026
Epoch [254/300], Loss: 2.975849
Epoch [255/300], Loss: 2.974819
Epoch [256/300], Loss: 2.972493
Epoch [257/300], Loss: 2.970764
Epoch [258/300], Loss: 2.971490
Epoch [259/300], Loss: 2.969854
Epoch [260/300], Loss: 2.964686
Epoch [261/300], Loss: 2.956352
Epoch [262/300], Loss: 2.961170
Epoch [263/300], Loss: 2.965878
Epoch [264/300], Loss: 2.956515
Epoch [265/300], Loss: 2.959013
Epoch [266/300], Loss: 2.956658
Epoch [267/300], Loss: 2.950287
Epoch [268/300], Loss: 2.950371
Epoch [269/300], Loss: 2.943804
Epoch [270/300], Loss: 2.944567
Epoch [271/300], Loss: 2.945858
Epoch [272/300], Loss: 2.943100
Epoch [273/300], Loss: 2.934236
Epoch [274/300], Loss: 2.938207
Epoch [275/300], Loss: 2.940143
Epoch [276/300], Loss: 2.936033
Epoch [277/300], Loss: 2.930204
Epoch [278/300], Loss: 2.928652
Epoch [279/300], Loss: 2.928629
Epoch [280/300], Loss: 2.932060
Epoch [281/300], Loss: 2.927207
Epoch [282/300], Loss: 2.921672
Epoch [283/300], Loss: 2.922609
Epoch [284/300], Loss: 2.923648
Epoch [285/300], Loss: 2.917558
Epoch [286/300], Loss: 2.914884
Epoch [287/300], Loss: 2.909767
Epoch [288/300], Loss: 2.914728
Epoch [289/300], Loss: 2.913143
Epoch [290/300], Loss: 2.911689
Epoch [291/300], Loss: 2.903575
Epoch [292/300], Loss: 2.905037
Epoch [293/300], Loss: 2.913927
Epoch [294/300], Loss: 2.902477
Epoch [295/300], Loss: 2.906380
Epoch [296/300], Loss: 2.896818
Epoch [297/300], Loss: 2.896481
Epoch [298/300], Loss: 2.896956
Epoch [299/300], Loss: 2.890868
Epoch [300/300], Loss: 2.893534
Traceback (most recent call last):
  File "train_model5.py", line 356, in <module>
    main()
  File "train_model5.py", line 346, in main
    generated_tokens = generate_packets(model, user_prompt, text_tokenizer, packet_tokenizer, device, max_len=100)
  File "train_model5.py", line 231, in generate_packets
    top_k_filtered_logits = top_k_logits(logits, k)
  File "train_model5.py", line 212, in top_k_logits
    return torch.where(logits < min_values, float('-inf'), logits)
RuntimeError: expected scalar type double but found float
